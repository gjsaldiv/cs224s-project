{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6iAqK_e-z75"
      },
      "source": [
        "# Imports \n",
        "import librosa\n",
        "import librosa.display\n",
        "import librosa.effects\n",
        "import librosa.util\n",
        "\n",
        "import numpy as np\n",
        "import sys, os\n",
        "import pandas as pd\n",
        "\n",
        "import sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Wt8EALFBXZn"
      },
      "source": [
        "# Would use similar \n",
        "# wav_agent, sr_agent = librosa.load('./sample/agent/0002f70f7386445b.wav')\n",
        "# wav_caller, sr_caller = librosa.load('./sample/caller/0002f70f7386445b.wav')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IEPrjp5AtlU"
      },
      "source": [
        "#Features we want right now: min f0, max f0, and mean f0 and maybe rms (not sure exactly what that is but was used in the paper)\n",
        "#more features: pitch range , \n",
        "path = './CREMA-D/AudioWAV/'\n",
        "files = os.listdir(path)\n",
        "\n",
        "summary = pd.read_csv('./CREMA-D/processedResults/summaryTable.csv')\n",
        "\n",
        "num_files = len(os.listdir(path)) #not sure how you want to count files\n",
        "count = 0\n",
        "X = np.zeros((num_files, 6))\n",
        "Y = np.zeros(num_files).astype(str)\n",
        "for sample in tqdm(files): #depends on how you access\n",
        "  file = os.path.join(path,sample) \n",
        "  current_wav, current_sr = librosa.load(file) #fix for set up \n",
        "  f0_series = librosa.yin(current_wav, librosa.note_to_hz('C2'), librosa.note_to_hz('C7'))\n",
        "  rms_series = librosa.feature.rms(y=current_wav)\n",
        "  f0_max = np.amax(f0_series)\n",
        "  f0_min = np.amin(f0_series)\n",
        "  f0_mean = np.mean(f0_series)\n",
        "  rms_max = np.amax(rms_series)\n",
        "  rms_min = np.amin(rms_series)\n",
        "  rms_mean = np.mean(rms_series)\n",
        "  x = np.array([f0_min, f0_max, f0_mean, rms_min, rms_max, rms_mean])\n",
        "  X[count,:] = x\n",
        "  info = summary.loc[summary['FileName'] == sample.split('.')[0]]\n",
        "  try:\n",
        "    Y[count] = info['VoiceVote'].values[0]\n",
        "  except Exception as ex:\n",
        "    print(f'info: {info}')\n",
        "    print(f'index count: {count}')\n",
        "    print(f'unable to find file: {sample}')\n",
        "  count+=1\n",
        "print(f'shape of train data: {X.shape}')\n",
        "print(f'shape of labels: {Y.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n60PZXht_tv-"
      },
      "source": [
        "#For Logistic Regression, can use sklearn.linear_model.LogisticRegression\n",
        "# !pip install -U scikit-learn\n",
        "# import sklearn\n",
        "# from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLduPvIOC7cb"
      },
      "source": [
        "# Get data and labels for training\n",
        "train_split = int(0.8 * num_files)\n",
        "print('train_split: ', train_split)\n",
        "print(f'train size: {X[:train_split].shape}, val size: {X[train_split:].shape}')\n",
        "\n",
        "model =  SVC()\n",
        "model.fit(X[:train_split], Y[:train_split])\n",
        "predictions = model.predict(X[train_split:])\n",
        "score = model.score(X[train_split:],Y[train_split:])\n",
        "print(f'Test accuracy score: {score}')\n",
        "\n",
        "f1 = f1_score(Y[train_split:], predictions, average='macro')\n",
        "print(f'macro f1 score: {f1}')\n",
        "f1 = f1_score(Y[train_split:], predictions, average='micro')\n",
        "print(f'micro f1 score: {f1}')\n",
        "\n",
        "model2 =  SVC(kernel=\"linear\")\n",
        "model2.fit(X[:train_split], Y[:train_split])\n",
        "predictions = model2.predict(X[train_split:])\n",
        "score = model2.score(X[train_split:],Y[train_split:])\n",
        "print(f'Test accuracy score, model2: {score}')\n",
        "\n",
        "f1 = f1_score(Y[train_split:], predictions, average='macro')\n",
        "print(f'macro f1 score, model2: {f1}')\n",
        "f1 = f1_score(Y[train_split:], predictions, average='micro')\n",
        "print(f'micro f1 score, model2: {f1}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6deWxsTtsp51"
      },
      "source": [
        "# Some basic stats for the dataset\n",
        "avg = np.mean(X, axis=0)\n",
        "stats = ['f0_min', 'f0_max', 'f0_mean', 'rms_min', 'rms_max', 'rms_mean']\n",
        "for j,stat in enumerate(stats):\n",
        "    print(f'{stat} average: {avg[j]}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}