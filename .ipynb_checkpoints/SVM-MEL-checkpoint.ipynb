{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S6iAqK_e-z75"
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.effects\n",
    "import librosa.util\n",
    "\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_wav(wav, wav_max_length, pad=0):\n",
    "  \"\"\"Pads audio wave sequence to be `wav_max_length` long.\"\"\"\n",
    "  dim = wav.shape[1]\n",
    "  padded = np.zeros((wav_max_length, dim)) + pad\n",
    "  if len(wav) > wav_max_length:\n",
    "    wav = wav[:wav_max_length]\n",
    "  length = len(wav)\n",
    "  padded[:length, :] = wav\n",
    "  return padded, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "5IEPrjp5AtlU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 5954/7442 [07:57<01:55, 12.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info: Empty DataFrame\n",
      "Columns: [Unnamed: 0, FileName, VoiceVote, VoiceLevel, FaceVote, FaceLevel, MultiModalVote, MultiModalLevel]\n",
      "Index: []\n",
      "index count: 5952\n",
      "unable to find file: 1040_ITH_SAD_X.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7442/7442 [09:57<00:00, 12.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train data: (7442, 32006)\n",
      "shape of labels: (7442,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Features we want right now: min f0, max f0, and mean f0 and maybe rms (not sure exactly what that is but was used in the paper)\n",
    "#more features: pitch range , \n",
    "#path = './CREMA-D/AudioWAV/'\n",
    "path = '/Users/gabesaldivar/Desktop/cs224s/CREMA-D/AudioWAV/'\n",
    "files = os.listdir(path)\n",
    "\n",
    "#summary = pd.read_csv('./CREMA-D/processedResults/summaryTable.csv')\n",
    "summary = pd.read_csv('/Users/gabesaldivar/Desktop/cs224s/CREMA-D/processedResults/summaryTable.csv')\n",
    "\n",
    "num_files = len(os.listdir(path)) #not sure how you want to count files\n",
    "count = 0\n",
    "X = np.zeros((num_files, (250 * 128) + 6))# or +8 for features\n",
    "Y = np.zeros(num_files).astype(str)\n",
    "for sample in tqdm(files): #depends on how you access\n",
    "  file = os.path.join(path,sample) \n",
    "  current_wav, current_sr = librosa.load(file) #fix for set up \n",
    "  f0_series = librosa.yin(current_wav, librosa.note_to_hz('C2'), librosa.note_to_hz('C7'))\n",
    "  rms_series = librosa.feature.rms(y=current_wav)\n",
    "  f0_max = np.amax(f0_series)\n",
    "  f0_min = np.amin(f0_series)\n",
    "  f0_mean = np.mean(f0_series)\n",
    "  rms_max = np.amax(rms_series)\n",
    "  rms_min = np.amin(rms_series)\n",
    "  rms_mean = np.mean(rms_series)\n",
    "  mel_spec = librosa.feature.melspectrogram(current_wav, current_sr)\n",
    "  m_log = librosa.power_to_db(mel_spec)\n",
    "  m_log_norm = librosa.util.normalize(m_log)\n",
    "  padded_wav, input_length = pad_wav(m_log_norm.T, 250)\n",
    "  flat_mel = padded_wav.flatten()\n",
    "  x = np.array([f0_min, f0_max, f0_mean, rms_min, rms_max, rms_mean])\n",
    "  x = np.append(x, flat_mel)\n",
    "  #print('small x', x)\n",
    "  X[count,:] = x\n",
    "  info = summary.loc[summary['FileName'] == sample.split('.')[0]]\n",
    "  try:\n",
    "    Y[count] = info['VoiceVote'].values[0]\n",
    "  except Exception as ex:\n",
    "    print(f'info: {info}')\n",
    "    print(f'index count: {count}')\n",
    "    print(f'unable to find file: {sample}')\n",
    "  count+=1\n",
    "print(f'shape of train data: {X.shape}')\n",
    "print(f'shape of labels: {Y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n60PZXht_tv-"
   },
   "outputs": [],
   "source": [
    "#For Logistic Regression, can use sklearn.linear_model.LogisticRegression\n",
    "# !pip install -U scikit-learn\n",
    "# import sklearn\n",
    "# from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLduPvIOC7cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_split:  5953\n",
      "train size: (5953, 32006), val size: (1489, 32006)\n"
     ]
    }
   ],
   "source": [
    "# Get data and labels for training\n",
    "train_split = int(0.8 * num_files)\n",
    "print('train_split: ', train_split)\n",
    "print(f'train size: {X[:train_split].shape}, val size: {X[train_split:].shape}')\n",
    "\n",
    "model =  SVC()\n",
    "model.fit(X[:train_split], Y[:train_split])\n",
    "predictions = model.predict(X[train_split:])\n",
    "score = model.score(X[train_split:],Y[train_split:])\n",
    "print(f'Test accuracy score: {score}')\n",
    "\n",
    "f1 = f1_score(Y[train_split:], predictions, average='macro')\n",
    "print(f'macro f1 score: {f1}')\n",
    "f1 = f1_score(Y[train_split:], predictions, average='micro')\n",
    "print(f'micro f1 score: {f1}')\n",
    "\n",
    "'''model2 =  SVC(kernel=\"linear\")\n",
    "model2.fit(X[:train_split], Y[:train_split])\n",
    "predictions = model2.predict(X[train_split:])\n",
    "score = model2.score(X[train_split:],Y[train_split:])\n",
    "print(f'Test accuracy score, model2: {score}')\n",
    "\n",
    "f1 = f1_score(Y[train_split:], predictions, average='macro')\n",
    "print(f'macro f1 score, model2: {f1}')\n",
    "f1 = f1_score(Y[train_split:], predictions, average='micro')\n",
    "print(f'micro f1 score, model2: {f1}')'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix plot\n",
    "sklearn.metrics.plot_confusion_matrix(model, X[train_split:],Y[train_split:], )  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6deWxsTtsp51"
   },
   "outputs": [],
   "source": [
    "# Some basic stats for the dataset\n",
    "avg = np.mean(X, axis=0)\n",
    "stats = ['f0_min', 'f0_max', 'f0_mean', 'rms_min', 'rms_max', 'rms_mean']\n",
    "for j,stat in enumerate(stats):\n",
    "    print(f'{stat} average: {avg[j]}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
